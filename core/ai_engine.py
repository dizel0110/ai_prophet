import logging
import requests
import os
from google import genai
from google.genai import types as genai_types
from config import GEMINI_KEY, HF_TOKEN, SYSTEM_PROMPT, FALLBACK_MODELS, HF_TASKS

logger = logging.getLogger(__name__)

# –ö–ª–∏–µ–Ω—Ç Gemini
gemini_client = genai.Client(api_key=GEMINI_KEY)
_chats = {}

def get_ai_chat(chat_id, model_name=None):
    if not model_name: model_name = FALLBACK_MODELS[0]
    session_key = f"{chat_id}_{model_name}"
    
    if session_key not in _chats:
        try:
            _chats[session_key] = gemini_client.chats.create(
                model=model_name,
                config=genai_types.GenerateContentConfig(
                    system_instruction=SYSTEM_PROMPT,
                    temperature=0.7
                )
            )
            logger.info(f"üÜï AI Session Created: {session_key}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to init {model_name}: {e}")
            return None
    return _chats[session_key]

def get_hf_response(text=None, image_path=None, task="text"):
    if not HF_TOKEN: return "–û—à–∏–±–∫–∞: HF_TOKEN –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω."
    
    # –§–ï–í–†–ê–õ–¨ 2026: –ï–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∂–∏–≤–æ–π –∫–æ—Ä–µ–Ω—å –¥–ª—è –≤—Å–µ—Ö –∑–∞–¥–∞—á
    BASE_URL = "https://router.huggingface.co/hf-inference"
    model_id = HF_TASKS.get(task, HF_TASKS["text"])
    
    headers = {
        "Authorization": f"Bearer {HF_TOKEN}",
        "x-wait-for-model": "true"
    }
    
    try:
        # –î–ª—è –¢–ï–ö–°–¢–ê —Ä–æ—É—Ç–µ—Ä –ª—é–±–∏—Ç OpenAI —Ñ–æ—Ä–º–∞—Ç –∏–ª–∏ –ø—Ä—è–º–æ–π /models/
        if task == "text":
            api_url = f"{BASE_URL}/v1/chat/completions"
            payload = {
                "model": model_id,
                "messages": [{"role": "user", "content": f"{SYSTEM_PROMPT}\n\n{text}"}],
                "max_tokens": 500
            }
            response = requests.post(api_url, headers=headers, json=payload, timeout=60)
            if response.status_code == 200:
                logger.info(f"‚úÖ HF V1 Chat Success for {model_id}")
                return response.json()['choices'][0]['message']['content']
        
        # –î–ª—è –ú–ï–î–ò–ê (–§–æ—Ç–æ/–ì–æ–ª–æ—Å) - –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –ø—É—Ç—å –†–æ—É—Ç–µ—Ä–∞ /models/
        api_url = f"{BASE_URL}/models/{model_id}"
        
        if task == "vision" and image_path:
            with open(image_path, "rb") as f: data = f.read()
            response = requests.post(api_url, headers=headers, data=data, timeout=60)
        elif task == "audio" and image_path:
            with open(image_path, "rb") as f: data = f.read()
            # Whisper –Ω–∞ –†–æ—É—Ç–µ—Ä–µ —Ç—Ä–µ–±—É–µ—Ç —á–∏—Å—Ç–æ–≥–æ –ø–æ—Ç–æ–∫–∞
            response = requests.post(api_url, headers=headers, data=data, timeout=60)
        elif task == "text": # –ï—Å–ª–∏ V1 –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª
            response = requests.post(api_url, headers=headers, json={"inputs": text}, timeout=60)
        else:
            return None

        if response.status_code != 200:
            logger.error(f"‚ùå HF Router Error {response.status_code} for {model_id} at {api_url}: {response.text[:200]}")
            return None

        result = response.json()
        logger.info(f"‚úÖ HF {task} raw result received.")
        
        if isinstance(result, dict):
            return result.get('text', result.get('generated_text', str(result)))
        if isinstance(result, list) and len(result) > 0:
            item = result[0]
            if isinstance(item, dict):
                return item.get('text', item.get('generated_text', ''))
        return str(result)

    except Exception as e:
        logger.error(f"‚ùå HF Engine Exception: {e}")
        return None

def reset_chat(chat_id, model_name=None):
    if model_name: _chats.pop(f"{chat_id}_{model_name}", None)
    else:
        keys = [k for k in _chats if k.startswith(f"{chat_id}_")]
        for k in keys: _chats.pop(k, None)

def get_client():
    return gemini_client
