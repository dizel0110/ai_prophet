import logging
import requests
import os
import base64
from google import genai
from google.genai import types as genai_types
from config import GEMINI_KEY, HF_TOKEN, SYSTEM_PROMPT, FALLBACK_MODELS, HF_TASKS

logger = logging.getLogger(__name__)

# –ö–ª–∏–µ–Ω—Ç Gemini
gemini_client = genai.Client(api_key=GEMINI_KEY)
_chats = {}

def get_ai_chat(chat_id, model_name=None):
    if not model_name: model_name = FALLBACK_MODELS[0]
    session_key = f"{chat_id}_{model_name}"
    
    if session_key not in _chats:
        try:
            _chats[session_key] = gemini_client.chats.create(
                model=model_name,
                config=genai_types.GenerateContentConfig(
                    system_instruction=SYSTEM_PROMPT,
                    temperature=0.7
                )
            )
            logger.info(f"üÜï AI Session Created: {session_key}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to init {model_name}: {e}")
            return None
    return _chats[session_key]

def get_hf_response(text=None, image_path=None, task="text"):
    if not HF_TOKEN: return "–û—à–∏–±–∫–∞: HF_TOKEN –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω."
    
    model_id = HF_TASKS.get(task, HF_TASKS["text"])
    headers = {
        "Authorization": f"Bearer {HF_TOKEN}",
        "Content-Type": "application/json",
        "x-wait-for-model": "true"
    }
    
    try:
        # –ü–£–¢–¨ 2026: –í—Å–µ —á–µ—Ä–µ–∑ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π –†–æ—É—Ç–µ—Ä
        api_url = "https://router.huggingface.co/v1/chat/completions"
        
        if task == "vision" and image_path:
            with open(image_path, "rb") as f:
                encoded_image = base64.b64encode(f.read()).decode('utf-8')
            
            payload = {
                "model": model_id,
                "messages": [{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": text or "–û–ø–∏—à–∏ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞–∫ AI Prophet."},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{encoded_image}"}}
                    ]
                }],
                "max_tokens": 500
            }
        elif task == "audio" and image_path:
            # –î–ª—è –∞—É–¥–∏–æ –†–æ—É—Ç–µ—Ä 2026 —Ç—Ä–µ–±—É–µ—Ç multipart –∏–ª–∏ –ø—Ä—è–º–æ–π –ø—É—Ç—å –∫ v1/audio/transcriptions
            # –ù–æ —Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π - –ø—Ä—è–º–æ–π –¥–æ–º–µ–Ω Hugging Face v1
            audio_url = f"https://router.huggingface.co/v1/audio/transcriptions"
            with open(image_path, "rb") as f:
                files = {"file": f}
                audio_headers = {"Authorization": f"Bearer {HF_TOKEN}"}
                response = requests.post(audio_url, headers=audio_headers, files=files, data={"model": model_id}, timeout=60)
                if response.status_code == 200:
                    return response.json().get('text', '')
                else:
                    # Fallback to direct model path for binary
                    api_url = f"https://router.huggingface.co/hf-inference/models/{model_id}"
                    with open(image_path, "rb") as f: data = f.read()
                    response = requests.post(api_url, headers=audio_headers, data=data, timeout=60)
                    if response.status_code == 200:
                        res = response.json()
                        return res[0].get('text', '') if isinstance(res, list) else res.get('text', str(res))
                    return None
        else:
            # –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
            payload = {
                "model": model_id,
                "messages": [{"role": "user", "content": f"{SYSTEM_PROMPT}\n\n{text}"}],
                "max_tokens": 500
            }

        if task != "audio":
            response = requests.post(api_url, headers=headers, json=payload, timeout=60)
            if response.status_code == 200:
                result = response.json()
                logger.info(f"‚úÖ HF {task} success via Router V1")
                return result['choices'][0]['message']['content']
            else:
                logger.error(f"‚ùå HF Router Error {response.status_code}: {response.text[:200]}")
                return None

    except Exception as e:
        logger.error(f"‚ùå HF Engine Exception: {e}")
        return None

def transcribe_with_gemini(file_path):
    """–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Gemini –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ –∞—É–¥–∏–æ (—Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π –ø—É—Ç—å 2026)"""
    try:
        with open(file_path, 'rb') as f: bytes_data = f.read()
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Gemini 1.5 Flash –∫–∞–∫ —Å–∞–º—É—é —Å—Ç–∞–±–∏–ª—å–Ω—É—é –¥–ª—è –∞—É–¥–∏–æ-–∑–∞–¥–∞—á
        response = gemini_client.models.generate_content(
            model='gemini-1.5-flash',
            contents=[
                "–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–π —ç—Ç–æ –∞—É–¥–∏–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–æ. –ù–∞–ø–∏—à–∏ —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç.",
                genai_types.Part.from_bytes(data=bytes_data, mime_type='audio/ogg')
            ]
        )
        if response.text:
            logger.info("‚úÖ Gemini Audio Transcription Success")
            return response.text.strip()
    except Exception as e:
        logger.error(f"‚ùå Gemini Transcription Error: {e}")
    return None

def reset_chat(chat_id, model_name=None):
    if model_name: _chats.pop(f"{chat_id}_{model_name}", None)
    else:
        keys = [k for k in _chats if k.startswith(f"{chat_id}_")]
        for k in keys: _chats.pop(k, None)

def get_client():
    return gemini_client
